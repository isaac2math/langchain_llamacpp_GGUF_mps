{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chdir\n",
    "if platform.system() == 'Darwin':\n",
    "\n",
    "    print(\"GPTQ is designed for NV cards\")\n",
    "\n",
    "elif platform.system() == 'Linux':\n",
    "    \n",
    "    os.chdir(\"/home/ning/Data/Dropbox/Working_Directory/NLP/langchain_llama2\")\n",
    "    \n",
    "    print(\"=====================================================\")\n",
    "    print(\"Linux detected; PopOS recommended for GPU computing.\")\n",
    "    print(\"the current wd is :\")\n",
    "    print(os.getcwd())\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "else :\n",
    "    \n",
    "    print(\"I hate Windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/feature/test1_abs.txt', 'r+', encoding=\"utf-8\") as file:\n",
    "    context = file.read()\n",
    "# context = context[:726]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_or_path = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/Llama-2-7B-GPTQ\", use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\"./model/Llama-2-7B-GPTQ\",\n",
    "                                           model_basename=\"gptq_model-4bit-128g\",\n",
    "                                           use_safetensors=True,\n",
    "                                           trust_remote_code=True,\n",
    "                                           device=\"cuda:0\",\n",
    "                                           use_triton=use_triton,\n",
    "                                           quantize_config=None)\n",
    "\n",
    "prompt = \"who is the victim in the following article ? \"\n",
    "prompt_template=f'''{prompt} : {context}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
